{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p>CAVE! This documentation is still a work in progress, as VIBSO is in a very early stage of development. If you have any documentation related questions or suggestions on how to improve it, please feel free to file an issue using this template.</p> <p>Welcome to the documentation of the Vibrational Spectroscopy Ontology (VIBSO)!</p> <p>VIBSO is the first ontology to be developed by the NFDI4Chem project, which has as core aim to make research data FAIR. The scope of VIBSO is thus focused on the use case of providing the terminology with which research data can be annotated. Such annotations can either be in form of RDF knowledge graphs that use VIBSO terms directly within their triples, or the research data is formally described according to some metadata schema which keys are mapped to terms from VIBSO and compatible ontologies in order to provide a common semantic defining ground. Following an incremental ontology development approach, we are first going to focus on the domain of vibrational Raman spectroscopy before coming to other vibrational spectroscopy assays, such as infrared spectroscopy.</p>"},{"location":"#content","title":"Content","text":"<ul> <li>Domain Definition</li> <li>Competency Questions</li> <li>Design Patterns</li> <li>Development Approach</li> <li>Contributing to VIBSO</li> <li>Default ODK Workflows</li> <li>ODK based Repository Structure</li> <li>ODK based Continuous Integration</li> </ul>"},{"location":"cite/","title":"How to cite VIBSO","text":"<p>Please cite VIBSO by its GitHub URL <code>https://github.com/NFDI4Chem/VibrationalSpectroscopyOntology</code> and credit NFDI4Chem as well as VIBSO's contributors as creators.</p>"},{"location":"competency_questions/","title":"VIBSO's Competency Questions","text":"<p>Competency questions can be understood as the test cases with which one can check whether the ontology is in scope and covers the intended use case. By making statements by using the terms defined in the ontology including its imported external ones, the competency questions should be answerable.</p> <p>As previously mentioned, VIBSO's scope is the semantic description of vibrational spectroscopy research data. So far we have thus identified the following competency questions:</p>"},{"location":"competency_questions/#questions-addressing-one-single-data-set","title":"Questions addressing one single data set","text":"<ul> <li>What kind of vibrational spectroscopy was performed (as detailed as possible: enhanced, resonant, stimulated,     coherent, anti-Stokes...)?</li> <li>In what context was it performed, e.g. as part of what study or investigation?</li> <li>What devices were used?</li> <li>What is the software version?</li> <li>Whats the instrument manufacturer?</li> <li>What is the instrument model?</li> <li>What is the instrument license number?</li> <li>Which user ID was used to operate the instrument?</li> <li>What were the protocol parameters &amp; device configurations?</li> <li>What was the scan setting (e.g. range, CCD area x &amp; y binninig)?</li> <li>What is the file extension of the measurement file?</li> <li>What is the unit of the measurement?</li> <li>What file output format is the measurement in?</li> <li>What is the types of acquisition for this measurement?</li> <li>What was the humidity and temperature of the collection environment? </li> <li>What are the light conditions around the sample? (closed or open space)</li> <li>Which laser wavelength was used for the measurement?</li> <li>What grating was used for the spectrum acquisition/measurement?</li> <li>What exposure time was used during the measurement?</li> <li>What detector was used for the measurement and what pixel resolution does it have?</li> <li>What lens was used for the measurement?</li> <li>What was the laser power percentage at the sample?</li> <li>How many accumulations were used?</li> <li>What was the spatial resolution of the system?</li> <li>Did you correct for the dark spectrum of the detector? (ie noise correction from detector)</li> <li>What kind of filters do you use for your measurement?</li> <li>Which slit position was used? Standard or high confocality?</li> <li>Has the autofocus  been used? </li> <li>Have the data been acquired with an automatic cosmic ray removal?</li> <li>Who was involved in the assay? (identified by a PID like ORCID at best)</li> <li>What was the assayed sample, including its relevant attributes (e.g. substrate, production procedure, pre-treatment)?</li> <li>What is the dimensionality of the experiment (e.g. single spectra/static scan, line maps, surface maps, volume scans...)? </li> <li>If the spectrum is a static scan, what is the center of the spectrum?</li> <li>What kind of preprocessing was done to the spectra?</li> <li>What kind of preprocessing was done on the sample?</li> <li>What kind of post-processing (e.g. smoothing, baseline subtraction, band integration, peak recognition, image     reconstruction) was done?</li> <li>Do you perform correction for spectral shifts?</li> <li>What were the data extraction steps that have been done on the spectra?</li> <li>For baseline correction, what are the applied baseline correction methods?</li> <li>If the cosmic ray removal was applied manually, what are the applied processing steps to do the cosmic ray       removal?</li> <li>What kind of preprocessing was done on the sample?</li> <li>Has the SNR been maintained for all measurements at expected levels?</li> <li>Were there any errors encountered during the measurement?</li> </ul>"},{"location":"competency_questions/#questions-addressing-the-full-database","title":"Questions addressing the full database","text":"<ul> <li>Find me all Raman spectra of wolfram wires on a silicium substrate covering the range 600-1800 cm^-1.</li> <li>Find me which is a safe range of incoming laser powers to perform Raman experiments at wavelength Z on sample XY.</li> <li>Find me a suitable solvent which is compatible with sample XY, i.e. which does not have any Raman peak or band between wavenumbers W1 and W2. This list represents only a first draft and needs to be thoroughly discussed with the domain experts and the NFDI4Chem community. </li> </ul>"},{"location":"contributing/","title":"How to contribute to VIBSO","text":"<p>Thank you for being a part of the Open Science team and being interested in finding out how to help us! All that you need to know on how to contribute to VIBSO should be covered within this documentation, or in the links it provides. If this isn't the case please let us know.</p> <p>There are different ways in which you can help:</p> <ul> <li>Open up a new issue: For any kind of contributions we rely on the GitHub infrastructure for documentation. Thus, it is always preferred that you use one of our issue templates to let us know what needs to be done and how you might plan on contributing, before working on a pull request (PR). (However, feel free to do a quick PR patch for any typos off course.)<ul> <li>Use this template to file bugs within the ontology or its CI/CD pipeline,</li> <li>this one to make a new term request (but please read the How to add new terms to VIBSO section  first),</li> <li>or this one to help us improve this documentation and ask questions about VIBSO.</li> </ul> </li> </ul>"},{"location":"design_patterns/","title":"Design Patterns &amp; Decisions","text":"<p>This page will be updated regularly in the iterative development of VIBSO to document modeling decisions and design  patterns.</p> <p>Following best practices in ontology development, we will reuse established design patterns whenever possible.  Since most ontologies we reuse are OBO Foundry based, we also reuse their design patterns to be logically sound and  interoperable. This entails committing to its underlying ontological framework, which is on the most abstract level grounded in the Basic Formal Ontology (BFO) 2.0 classes only version and the current version of the Relation Ontology  (RO). These patterns are expressed formally in the logical definitions of the imported classes in form of asserted <code>rdfs:equivalentTo</code> or <code>rdfs:subclassOf</code> axioms and in the logical definitions of the imported properties in form of  asserted <code>rdfs:domain</code> and <code>rdfs:range</code> axioms and their type (e.g. transitive, functional, symmetric, etc.).  Although less formally binding, the textual definitions of the classes and relations constituting these pattern are  also part of their expression.</p>"},{"location":"design_patterns/#vibso-tbox","title":"VIBSO TBox","text":"<p>Here you can see VIBSO's current terminology box (TBox) that focuses on vibrational Raman spectroscopy and  which also shows how VIBSO depends on reusing existing ontology classes, relations and design patterns.</p> <p></p> <p>This TBox is supposed to be updated iteratively whenever a new term is being discussed for inclusion.  We hope that it might be easier for domain experts to define technical terms in this ontological framework by adding  them here first.</p> <p>A higher resolution HTML version of this TBox and the below discussed pattern graphs with links to the used terms in  the NFDI4Chem Terminology Service can be found here. The source file to edit these with the draw.io app  is here.</p>"},{"location":"design_patterns/#planned-process-pattern","title":"Planned Process Pattern","text":"<p>Within the above TBox we can find specializations of a central class defined in the Ontology for Biomedical  Investigations (OBI) which is called 'planned proces'. This class is central in the OBO framework because it provides a general pattern for defining all kinds of processes in  which some agent(s) act(s) according to a set of predetermined instructions in order to achieve a certain outcome.  Logically, it is defined as a \"process that realizes a plan which is the concretization of a plan specification\".  Following the formalization of this definition, we can safely ignore the very abstract 'plan' class and only need to  focus on the 'plan specification'  that gets concretized by a 'plan'. A 'plan specification' is defined in the Information Artifact Ontology (IAO) as the  directive information that specifies through its parts which actions have to be performed ('action specification') in order to achieve the intended goals ('objective specification') of a 'planned process'. It thus resembles more fittingly what we usually mean in natural language when speaking of a  plan or method. To have a direct relation from a 'plan specification' to its 'planned proces', we can reuse  'executes' from the Statistical Methods Ontology (STATO), as it is defined exactly for this purpose.</p> <p>The pattern can be visualized like this:</p> <p></p> <p>We could apply this very general pattern for example to describe the baking of cookies like this:</p> <p></p>"},{"location":"design_patterns/#obi-core-assay-pattern","title":"OBI Core Assay Pattern","text":"<p>As defining all relevant types of vibrational spectroscopy assays, including their different kinds of data outputs  and their experimental setups from a research data management perspective is the central goal of VIBSO, we are  always dealing with special kinds of planned processes. For this we can leverage the fact that there also already  exists an 'assay'  class within OBI, which we can reuse as the central parent class for all vibrational spectroscopy assays.  It is defined as:</p> <pre><code> A planned process that has the objective to produce information \n about a material entity (the evaluant) by examining it.\n</code></pre> <p>This definition, along with the logical axioms asserted on this class and their entailments, constitute a core  design pattern which can be represented graphically like this:</p> <p></p> <p>To exemplify this pattern within the domain of VIBSO we can instantiate these classes like this.</p> <p></p> <p>To better understand the defined logic of this pattern, it seems useful to look at its parts more closely.</p>"},{"location":"design_patterns/#subjects-of-an-assay","title":"Subjects of an Assay","text":"<p>As we can see in this graph, the use of the term 'assay' is restricted to only those planned processes that examine a  'material entity' via the 'has specified input' relation. A material entity is defined in BFO as anything  that is extended in three-dimensional space, has a unique identity and contains some portion of matter. This could  include objects identified by their mass, material boundaries, or energy. Therefore, a rock, a nostril, or an elementary  particle such as an electron or quark, all qualify as a material entity. This restriction on the meaning of assay is not  just given in the above definition, but also in the definitions of 'assay objective' and 'evaluant role'.  Therefore, a planned process which examines a thing that is not a material entity cannot be classified as an assay in  this sense, its role can never be an 'evaluant role' and its objective can never be an 'assay objective'.</p>"},{"location":"design_patterns/#outputs-of-an-assay","title":"Outputs of an Assay","text":"<p>When looking at the output of an 'assay', we see that, although its textual definition only speaks of information in  general, the axiomatization in this pattern rather restricts the possible output to 'data item', a special kind  of information, via the 'has specified output'  relation. The 'data item' class is defined in IAO as a kind of information \"that is intended to be a truthful  statement about something [...] and is constructed/acquired by a method which reliably tends to produce (approximately)  truthful statements.\" Although this definition might suggest otherwise, an editor note on this class and the synonym  'data' make clear that 'data item' also includes the kinds of information that are compositions of multiple truthful  statements about a thing. In the context of the 'assay' pattern, we can thus say that an 'assay' must produce a truthful statement or a composition of truthful statements about the examined 'material entity'. To apply this to an example from  VIBSO's domain, we can say that a Raman spectrum is a 'data item' because it is composed of the measurements (e.g.  scattered light intensity values) that were recorded during a Raman spectroscopy assay.</p>"},{"location":"design_patterns/#defining-an-assay-according-to-the-obi-core-assay-pattern","title":"Defining an Assay According to the OBI Core Assay Pattern","text":"<p>There is an axiom asserted on the assay class which states that whenever an 'assay objective' is achieved by the use of  a planned process, this process must be classified as an 'assay'. Hence, the 'assay objective' must be the information  that describes the assay type by specifying what kind of data about the evaluated material is to be produced.  Accordingly, the definition of 'assay objective' reads: \"an objective specification to determine a specified type of  information about an evaluated entity (the material entity bearing evaluant role)\". However, 'analyte measurement  objective' seems to be the only subclass of 'assay objective' in OBO ontologies, and it is only used to group roughly 600 kinds of  assays as an 'analyte assay', which produces information about the presence, concentration, or amount of a  molecular entity or an atom, within the evaluated material. These analyte assays themselves are formally stratified  mostly by specifying which kind of molecular entity or atom is being analysed within the evaluated material. The  remaining 1000+ specializations of assay in OBI are formally stratified either by focusing on the evaluated material,  a certain characteristic of the evaluated material, the specific kind of data output, the devices used, or a  combination of these. </p> <p>This shows that we can use various axis of differentiation within the core assay pattern to classify a specific  kind of assay. Using logical axioms in the formal definition of assay types helps alot, as it allows multiple  inheritance to be inferred automatically by reasoners, an example of which is '3D molecular structure determination assay of an antigen:antibody complex'. What some assay definitions in OBI also indicate is that we sometimes need to extend the core OBI assay pattern  to classify certain assay types. Especially, if we want to differentiate or group assays in more detail, like when we  need more information about the used method including the devices and device settings.</p>"},{"location":"design_patterns/#extending-the-obi-core-assay-pattern","title":"Extending the OBI Core Assay Pattern","text":"<p>Since 'assay' is a specialization of 'planned process', we know that its method (plan) must be defined in some kind of  'plan specification'. In OBI, we find the class 'investigation assay specification', which is defined as a  'plan specification' that indicates the assay type, and which is also called 'Project Method'.  Some provided examples for this class are: sequence or mass spectrometry. Hence, we should be using this subclass of  'plan specification' in our assay pattern, and we can assert that an essential part of it is an 'assay objective'.  Similarly, we can say that the information which determines the experimental setup of a specific assay, such  as the needed devices, their settings or the way in which the evaluated sample must be prepared, is also an essential  part of the 'investigation assay specification'.</p>"},{"location":"design_patterns/#assay-devices","title":"Assay Devices","text":"<p>To represent the instruments used in an assay, we can import the already existing 'device' class from OBI and link  it to 'assay' via the 'has specified input' relation. The definition of this relation makes clear that the range  of it, in our case a 'device', must be specified in a 'plan specification', in our case an 'investigation assay  specification'.</p>"},{"location":"design_patterns/#assay-device-settings","title":"Assay Device Settings","text":"<p>To represent the part of an 'investigation assay specification' that is the information which specifies what kind of  configurable qualities an assay device must have, we can use the class 'setting datum', since it is defined as a  'data item' about \"some configuration of an instrument\". To represent the configurable device quality  specified by a setting datum, we can use the class 'physical object quality' from the Phenotype and Trait  Ontology (PATO), as it is the parent class of more specific physical qualities, such as temperature, wavelength, mass or  position. This quality can be related to the device via the 'quality of' relation from BFO. Since the actual quantitative value of the 'physical object quality' specified by a 'setting datum' can be encoded  using different units of measurement, we also need a way to formally represent this. The way this is currently done  in OBI is via the 'value specification' class, which is defined as an information \"that specifies a value within  a classification scheme or on a quantitative scale\". We can link this class to a 'setting datum' via the  'has value specification' relation which is a specialization of the very general 'has part' relation. For the  part of this value encoding that represents a given measurement unit, such as any SI unit, we can use one of the  instances of the class 'unit' defined in the Unit Ontology (UO), and link it via the  'has measurement unit label' relation from IAO. The last missing pieces are the links between a value  specification and the actual literal value respectively the configurable device quality. Both are defined in  OBI as 'has specified value', respectively 'specifies value of'.</p> <p>Having described all the classes and relations we need to extend the OBI core assay pattern, we can now visualize  this in from of the following graph: </p> <p>When comparing this graph to the previous one, one can see that we have also added an 'is about' relation between 'assay objective' and 'data item'. This relation was not added to the core pattern previously, as it is not formally  asserted in OBI, but only given in the textual definition of the 'assay objective'. </p> <p>To exemplify this extended pattern in the context of VIBSO, we can instantiate its classes as follows: </p>"},{"location":"design_patterns/#defining-an-assay-according-to-the-extended-assay-pattern-in-the-context-of-vibso","title":"Defining an Assay According to the Extended Assay Pattern in the Context of VIBSO","text":"<p>As discussions with domain experts in the VIBSO development calls have made clear, the classification of  the various types of Raman spectroscopy as currently done in CHMO has some problems (see issue#103). All of these types have in common that they somehow record the Raman scattering of the evaluated sample.  However, their methodology differs in the ways in which the Raman scattering is produced, recorded or subsequently  processed, by using either different devices, device settings, sample preparation steps or signal recording steps.  We thus need the extended assay pattern described here, to be able to describe these different methods in more  detail and thereby formally define the various types of Raman spectroscopy assays they specify. </p>"},{"location":"design_patterns/#the-assay-pattern-in-a-bigger-picture","title":"The Assay Pattern in a Bigger Picture","text":"<p>This assay pattern can of course combined with other planned process patterns, to be able to describe its embedding  in a grander context. Without going into the details, the following TBox shows how this can be done with respect the  competency questions:</p> <ul> <li>Who performed the assay?</li> <li>What kind of data transformations where performed on the data output of the assay?</li> <li>What kind of sampling process was done before the assay?</li> <li>What is the investigation the assay is a part of?</li> </ul> <p></p>"},{"location":"design_patterns/#using-example-instances-in-vibso","title":"Using Example Instances in VIBSO","text":"<p>In the previous VIBSO development calls, it became apparent that the proper use of the design patterns described  here should be illustrated with concrete example instances. This should not only make it easier for the domain  experts with little knowledge on ontology development to follow. We also hope to be able to spot logical inconsistencies  when minting new classes and asserting their class restrictions by using reasoners, such as ELK or HERMIT during the  development.</p> <p>We have thus added the ROBOT TSV template vibso_examples.tsv, to be able to add example instances for each class minted in VIBSO. Similar to the other ROBOT templates we use,  this one gets automatically converted into an OWL component (vibso_examples.owl) that is being imported into the  main editor file (vibso-edit.owl).</p>"},{"location":"design_patterns/#obi-quality-quantity-pattern","title":"OBI Quality &amp; Quantity Pattern","text":"<p> Since we are reusing OBO ontologies and their patterns, we also try to reuse OBI's way of modeling  data and values. Please read their documentation for more  background. Using this OBI pattern allows us to differentiate data values of qualities (aka attributes) of a material  entity, such as the spectroscope or sample, into data values that represent settings and those that represent  measurements. From a data repository use case perspective, we might not need this fine-grained approach and defining  qualities/attributes and their value specifications might suffice. Yet in order to allow the integration of VIBSO in  Electronic Lab Notebooks, such a differentiation will most likely be very useful. Although this pattern seems to  work in many OBO use cases, we need to see, if we have to adjust it for our needs. Other ontologies like QUDT or SIO  use slightly different patterns to model qualities and their quantitative representations.</p>"},{"location":"design_patterns/#measurement-example-kg-of-the-assay-pattern","title":"Measurement Example KG of the Assay Pattern","text":"<p>Lars Vogt and Tobias Kuhn demonstrate the use of the OBO assay pattern within a grander research context as follows (cited from their preprint DOI:10.13140/RG.2.2.13742.59203, p.8): </p> <p>Figure 3: A detailed machine-actionable representation of the metadata relating to a weight measurement datum documented as an RDF ABox graph. The representation takes the form of an ABox semantic graph following the RDF syntax. The graph documents a mass measurement process using a balance. It relates an instance of mass measurement assay (OBI:0000445) with instances of various other classes from different ontologies, specifying who conducted the measurement, where and when it took place, following which protocol and using which device (i.e., balance). The graph furthermore specifies the particular material entity that served as subject and thus as input of the measurement process (i.e., \u2018objectX\u2019), and it specifies the data that is the output of the observation, which is contained in a particular weight measurement assertion.</p>"},{"location":"development_approach/","title":"Development Approach","text":"<p>The development of VIBSO is an iterative, version-controlled one that relies on continuous integration. This means that VIBSO uses the Ontology Development Kit (ODK) and GitHub  for the technical implementation, making sure all changes are properly tracked and quality controlled.  With regard to the conceptual iterative development, VIBSO relies on the collaboration with domain experts from chemistry and related scientific fields in which vibrational spectroscopies are being used. They are the ones who know best what concepts are needed in the domain of VIBSO and how to properly label and define them. So far, the domain experts who contributed to VIBSO come from the institutions that are directly associated with the NFDI4Chem project. These domain experts provided an initial list of terms that was discussed with the VIBSO ontology curators in multiple online meetings and which was used to develop a first draft of a terminology box. (see Vibrational Raman Spectroscopy). Within these discussions it became apparent that the best way for allowing domain experts to directly contribute to VIBSO in a controlled way is to provide a tabular way to edit the ontology as well as to have a proper documentation as a binding development guideline.</p>"},{"location":"development_approach/#template-based-development","title":"Template Based Development","text":"<p>To enable an easier participation of domain experts, who often might have little to no background in the development of ontologies and the tools needed for this, we are using ROBOT TSV templates to define new classes (vibso_classes.tsv) and relations (vibso_object_properties.tsv), when there is no equivalent in other compatible OBO ontologies that we can reuse. The first row of these TSV table templates defines the human-readable header and the second row is the header needed by the ROBOT tool to properly convert the template into the OWL files that make up the components of VIBSO.</p>"},{"location":"development_approach/#explanation-of-the-template-column-headers","title":"Explanation of the template column headers","text":"<p>In both TSV templates there are the following columns:</p> <ul> <li>id: This column defines the identifier of a term in form of a CURIE (compact URI) which will be expanded into the full IRI of the term within the release artifacts. This unique id must always be in the syntax of <code>obo:VIBSO_[XXXXXXX]</code> whereas the seven <code>X</code> represented numerical digits. When the team of VIBSO curators grows and the need of editing in multiple TSV templates arises, we will probably set up workflows to ensure no mix-ups or duplicated IDs by assigning certain ID ranges to templates and/or contributors. For now, it should suffice to make sure there is no error or duplicate within these two TSVs.</li> <li>label: This column must contain either the agreed upon technical term of the concept to be defined, or, in cases of a completely new concept, it should be a singular word or phrase in lowercase that expresses the ontological type of the concept as human-readable as possible (for more on naming conventions see OBO principle #12).</li> <li>NOTE: When a term is to be deprecated, the prefix <code>deprecated_</code> should be added in front of the previous label to make this term status better visible to the users of the ontology.</li> <li>link(s) to issue(s): This column should be used to provide the link or links (separated with \"|\") to the issues in which this term is/was requested &amp; discussed (e.g. https://github.com/NFDI4Chem/VibrationalSpectroscopyOntology/issues/39).</li> <li>definition: A textual definition provides a human-readable understanding about the defined concept (class or relation). Textual definitions are, optimally, in concordance with associated machine-readable logical definitions (the latter of which are OPTIONAL). For more about writing good textual definitions please see OBO principle #6.</li> <li>definition source URI: It is common practice to provide a definition source whenever possible. In the best case this is the DOI of a publication in which the concept is explained and/or defined. URLs to encyclopedias like Wikipedia are also an option. Least preferred but still better than nothing would be the ORCID of the person who created the term.</li> <li>type: This column defines the type of the defined concept according to the Ontology Web Language (OWL) specification. The allowed values are: <code>owl:Class</code>, <code>owl:ObjectProperty</code>,  <code>owl:DatatypeProperty</code> or <code>owl:NamedIndividual</code>. So in the vibso_classes.tsv this should always be <code>owl:Class</code> and in the vibso_object_properties.tsv it should always be <code>owl:ObjectProperty</code>.</li> <li>Note: For OBO compliant ontologies, relations between classes (ObjectProperties) are meant to be defined mainly in RO and relations for data literals (DatatypeProperties) are only used very sparsely in order to not lose semantic depth.</li> <li>contributor URI: This column should be used to provide the ORCID of the people (separated with \"|\") who contributed to the curation of the term, in order to properly credit this micro contribution (e.g. <code>ORCID:0000-0002-1595-3213|ORCID:0000-0001-7694-5519|ORCID:0000-0002-2239-3955</code>).</li> <li>example of usage: This column should be used to provide a real world example of the term. This can either be done by providing exemplary triple statements or by elaborating how the term should be used in a concrete use case.  </li> <li>editor note: This column can be used to provide additional information regarding the term, such as summarizing a discussion about its definition or obsolescence reason.</li> <li>obsoleted: This column must only be used to provide the boolean value <code>true</code>, iff the term has been deprecated.</li> <li>obsolescence reason: This column should be used to provide an obsolescence reason from the controlled list of the OBO Metadata Ontology (OMO). The current allowed values are: <code>failed exploratory term , placeholder removed , terms merged , term imported , term split , out of scope</code></li> <li>term replaced by: This column should be used to provide the term that replaces a deprecated term. This is important to ensure backwards compatibility.</li> </ul> <p>In the vibso_classes.tsv, there is also the column:</p> <ul> <li>super class: This column is to be used to provide the parent of the owl:Class that is being defined. It is fine to use the label of the parent class here instead of its IRI or CURIE, as long as the class is already an imported term.</li> </ul> <p>In the vibso_classes.tsv, there are also the columns:</p> <ul> <li>super property: This column is to be used to provide the parent of the <code>owl:ObjectProperty</code> or <code>owl:DatatypeProperty</code> that is being defined, via its label.</li> <li>inverse property: This column can be used to define the inverse of an <code>owl:ObjectProperty</code>, via its label. </li> <li>domain: This column can be used to restrict an <code>owl:ObjectProperty</code> in terms of being allowed to be used only on a certain subject.</li> <li>range: This column can be used to restrict an <code>owl:ObjectProperty</code> in terms of being allowed to be used only on a certain object.</li> </ul> <p>The TSV templates will most likely be expanded in the future to also include more annotation properties (e.g. term status, comment) that will help document the ontology in its source code.</p>"},{"location":"development_approach/#proposed-contribution-workflow","title":"Proposed contribution workflow","text":"<p>VIBSO is an Open Science project from and for the scientific community! </p> <p>Thus, its development is aimed at allowing a broad range of contributions from stakeholders with different backgrounds. For this, the basic idea is to have everything needed within the GitHub repository and to use the ODK &amp; GitHub based workflows (e.g. issues &amp; pull requests) to propose and discuss any changes. This means that design patterns and decisions will have to be documented here as early as possible.</p> <p>As VIBSO is still at a very early stage, we will need to establish a team of domain experts that are frequently contributing and of whom some can serve as reviewers of open pull requests to assure a high quality. We envision to have regular virtual meetings in which open issues, pull request or questions can be discussed alongside the overall organization of the VIBSO development. </p> <p>For a more detailed description on how to contribute please continue reading here.</p>"},{"location":"development_approach/#are-you-an-expert-in-the-domain-of-vibrational-spectroscopy","title":"Are you an expert in the domain of vibrational spectroscopy?","text":"<ul> <li>You have first-hand experiences with doing vibrational spectroscopy assays, such as knowing the needed devices and their configuration parameters?</li> <li>You know how to write issues and do easy pull requests on file formats like .tsv, .md and .txt in GitHub or GitLab?</li> <li>You can imagine helping us to define the technical terms of vibrational spectroscopy (e.g. Raman &amp; IR) and collecting credits for such mirco-publications?</li> </ul> <p>Then we, a friendly and inclusive open science team, would like to invite you to be part of the VIBSO team &amp; development! Please get in touch with us via GitHub or <code>mailto:helpdesk@nfdi4chem.de</code></p>"},{"location":"domain_definition/","title":"Domain Definition","text":"<p>VIBSO's scope is to provide the semantic terms needed to describe experiments and data  within the domain of vibrational spectroscopy. With VIBSO it should be possible to make statements about the planning and realization of a vibrational spectroscopy assay, which is defined as a spectroscopic method that probes the vibrational modes of molecules or crystals. More specifically, VIBSO is intended to be used for making statements about who did what kind of vibrational spectroscopy, with what kind of sample, device and device configuration, under what lab protocol, in the context of what investigation as well as producing what kind of outputs. A more detailed description of VIBSO's competency questions can be found here.</p>"},{"location":"domain_definition/#ontological-dependencies","title":"Ontological Dependencies","text":"<p>Following the OBO Foundry Principles, VIBSO's domain coverage depends heavily on reusing many terms from existing ontologies instead of defining them anew. First and foremost it depends on the Basic Formal Ontology (BFO) as a common ground for abstract upper level classes such as material entity or process and on the Relation Ontology (RO) for commonly used relations. The Ontology of Biomedical Investigations (OBI) is another important and more concrete dependency of VIBSO, as it already provides many general classes and some specific relations within the domain of scientific investigations, such as assay, device or protocol. For more on this modular approach see the section on Design Patterns &amp; Decisions. An exact list of the ontologies from which domain relevant terms are imported can be found here. And more about the way this is done is documented here. </p> <p>Most importantly, VIBSO depends on classes from the Chemical Methods Ontology (CHMO), which already defines branches for the main chemical methods of interest - vibrational spectroscopy and  Raman spectroscopy - as well as other general classes needed to cover VIBSO's scope, such as spectrum or spectrometer. However, we have identified gaps within CHMO with regard to the domain of RDM. Also, the above-mentioned spectroscopic branches need to be vetted by domain experts taking into consideration previous work done by the IUPAC (see DOI:10.1515/pac-2019-0203) and in the now orphaned ontologies REX &amp; FIX, in which we can also find classes like vibrational spectroscopy or vibrational relaxation.</p> <p>Following best practices, we are collaborating with the developers and maintainers of CHMO and due to the domain specific overlap between it and VIBSO, it might be possible in the future that VIBSO will be somehow integrated into CHMO. At the moment however, it seems best to keep the two separated to address the identified gaps and issues regarding VIBSO's scope.</p>"},{"location":"ntr_workflow/","title":"Adding Terms to VIBSO","text":"<p>Following best practices in ontology development, we are reusing already existing terms from external ontologies whenever possible. This means that before requesting a new term in VIBSO, contributors are expected to research whether the term in question does not already exist in a compatible external ontology using a look-up portal, such as the NFDI4Chem Terminology Service or EBI's OLS.</p> <p>Chris Mungall wrote a really helpful blog post on \"How to select and request terms from ontologies\". The gist of it is represented in the below graphic taken from this blog post:</p> <p></p>"},{"location":"ntr_workflow/#importing-terms","title":"Importing terms","text":"<p>Whenever you identify a term missing in VIBSO and you have found a suitable candidate within an existing ontology, please feel free to open a new blank issue with a short but precise title (e.g. import OBI:microscope) and an explanation why you think this term should be imported into VIBSO with regard to its scope. We can thus discuss in the comment section of the issue, whether this term really is a good fit to be imported or why it might not be.</p> <p>Once it is decided to be needed, a pull request (PR) can be made to include it in the imports. For this it needs either be added in one of the existing text files where such term imports get declared (see /src/ontology/imports) and from which the OWL import modules are build automatically, according to the standard import workflow documented in here. If the term is from an ontology that is not yet being used as an import module in VIBSO the managing imports workflow needs to be triggered.</p>"},{"location":"ntr_workflow/#new-term-requests","title":"New Term Requests","text":"<p>When you've identified a term that is needed within the scope of VIBSO and which is not yet defined in an existing compatible ontology, a new term request (NTR) should be filed in VIBSO's issue tracker. This issue should then be used to engage the community in discussing the label, definition, position in the hierarchy as well as possible axiomatization. Once the discussion has reached a consensus, someone from the VIBSO team or maybe you, will have to make a PR in which the discussed changes to VIBSO will be implemented and can be reviewed. </p> <p>Please check the open NTR issues to make sure none exists for this term already. If it does, feel free to participate in the discussion. Else, please use this issue template to file your NTR.</p>"},{"location":"ntr_workflow/#adding-new-terms-to-the-tsv-template","title":"Adding New Terms to the TSV Template","text":"<p>Once a NTR is ready to be implemented in a PR, all you need to do is editing the vibso_terms.tsv template. For an explanation of the TSV columns, go here.</p>"},{"location":"ntr_workflow/#ontology-editing-principles-and-best-practices","title":"Ontology editing: principles and best practices","text":"<p>For more information on best practices in ontology editing, here are some really good links:</p> <ul> <li>OBO Foundry Principles </li> <li>OBOOK</li> </ul>"},{"location":"odk-workflows/","title":"Default ODK Workflows","text":"<ul> <li>Daily Editors Workflow</li> <li>Release Workflow</li> <li>Manage your ODK Repository</li> <li>Setting up Docker for ODK</li> <li>Imports management</li> <li>Managing the documentation</li> <li>Managing your Automated Testing</li> </ul>"},{"location":"odk-workflows/ContinuousIntegration/","title":"Introduction to Continuous Integration Workflows with ODK","text":"<p>Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding  this to your configuration file (src/ontology/vibso-odk.yaml):</p> <pre><code>ci:\n  - github_actions\n</code></pre> <p>When updateing your repo, you will notice a new file being added: <code>.github/workflows/qc.yml</code>.</p> <p>This file contains your CI logic, so if you need to change, or add anything, this is the place!</p> <p>Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding  this to your configuration file (src/ontology/vibso-odk.yaml):</p> <pre><code>ci:\n  - gitlab-ci\n</code></pre> <p>This will add a file called <code>.gitlab-ci.yml</code> in the root of your repo.</p>"},{"location":"odk-workflows/EditorsWorkflow/","title":"Editors Workflow","text":"<p>The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows:</p> <ol> <li>Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns.</li> <li>Completely automated data pipeline (GitHub Actions)</li> <li>DROID workflow</li> </ol> <p>This document only covers the first editing workflow, but more will be added in the future</p>"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","title":"Local editing workflow","text":"<p>Workflow requirements:</p> <ul> <li>git</li> <li>github</li> <li>docker</li> <li>editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc</li> </ul>"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","title":"1. Create issue","text":"<p>Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.</p>"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","title":"2. Update main branch","text":"<p>In your local environment (e.g. your laptop), make sure you are on the <code>main</code> (prev. <code>master</code>) branch and ensure that you have all the upstream changes, for example:</p> <pre><code>git checkout main\ngit pull\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","title":"3. Create feature branch","text":"<p>Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases)</p> <p>On your command line, this looks like this:</p> <pre><code>git checkout -b issue23removeprocess\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","title":"4. Perform edit","text":"<p>Using your editor of choice, perform the intended edit. For example:</p> <p>Prot\u00e9g\u00e9</p> <ol> <li>Open <code>src/ontology/vibso-edit.owl</code> in Prot\u00e9g\u00e9</li> <li>Make the change</li> <li>Save the file</li> </ol> <p>TextEdit</p> <ol> <li>Open <code>src/ontology/vibso-edit.owl</code> in TextEdit (or Sublime, Atom, Vim, Nano)</li> <li>Make the change</li> <li>Save the file</li> </ol> <p>Consider the following when making the edit.</p> <ol> <li>According to our development philosophy, the only places that should be manually edited are:<ul> <li><code>src/ontology/vibso-edit.owl</code></li> <li>Any ROBOT templates you chose to use (the TSV files only)</li> <li>Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns)</li> <li>components (anything in <code>src/ontology/components</code>), see here.</li> </ul> </li> <li>Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere.</li> <li>Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.</li> </ol>"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","title":"4. Check the Git diff","text":"<p>This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking.</p> <p>In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line:</p> <pre><code>git status\ngit diff\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","title":"5. Quality control","text":"<p>Now it's time to run your quality control checks. This can either happen locally (5a) or through your continuous integration system (7/5b).</p>"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","title":"5a. Local testing","text":"<p>If you chose to run your test locally:</p> <p><pre><code>sh run.sh make IMP=false test\n</code></pre> This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add <code>PAT=false</code> to skip the potentially lengthy process of rebuilding the patterns.</p> <pre><code>sh run.sh make IMP=false PAT=false test\n</code></pre>"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","title":"6. Pull request","text":"<p>When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example:</p> <pre><code>git add NAMEOFCHANGEDFILES\ngit commit -m \"Added biological process term #12\"\ngit push -u origin issue23removeprocess\n</code></pre> <p>Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls</p> <p>There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: <code>see #23</code> to link to a related ticket, or <code>fixes #23</code> if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).</p>"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","title":"7/5b. Continuous Integration Testing","text":"<p>If you didn't run and local quality control checks (see 5a), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions</p> <p>More on how to set this up here. Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.</p>"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","title":"8. Community review","text":"<p>Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this.</p> <p>This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!</p>"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","title":"9. Merge and cleanup","text":"<p>When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.</p>"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","title":"10. Changelog (Optional)","text":"<p>It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).</p>"},{"location":"odk-workflows/ManageDocumentation/","title":"Updating the Documentation","text":"<p>The documentation for VIBSO is managed in two places (relative to the repository root):</p> <ol> <li>The <code>docs</code> directory contains all the files that pertain to the content of the documentation (more below)</li> <li>the <code>mkdocs.yaml</code> file contains the documentation config, in particular its navigation bar and theme.</li> </ol> <p>The documentation is hosted using GitHub pages, on a special branch of the repository (called <code>gh-pages</code>). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually. All changes to the docs happen inside the <code>docs</code> directory on the <code>main</code> branch.</p>"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","title":"Editing the docs","text":""},{"location":"odk-workflows/ManageDocumentation/#changing-content","title":"Changing content","text":"<p>All the documentation is contained in the <code>docs</code> directory, and is managed in Markdown. Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow:</p> <ol> <li>Open the <code>.md</code> file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT: Do not edit any files in the <code>docs/odk-workflows/</code> directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker.</li> <li>Perform the edit and save the file</li> <li>Commit the file to a branch, and create a pull request as usual. </li> <li>If your development team likes your changes, merge the docs into main branch.</li> <li>Deploy the documentation (see below)</li> </ol>"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","title":"Deploy the documentation","text":"<p>The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps:</p> <ol> <li>In your terminal, navigate to the edit directory of your ontology, e.g.:    <pre><code>cd vibso/src/ontology\n</code></pre></li> <li>Now you are ready to build the docs as follows:    <pre><code>sh run.sh make update_docs\n</code></pre> Mkdocs now sets off to build the site from the markdown pages. You will be asked to<ul> <li>Enter your username</li> <li>Enter your password (see here for using GitHub access tokens instead)   IMPORTANT: Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens!</li> </ul> </li> </ol> <p>If everything was successful, you will see a message similar to this one:</p> <p><pre><code>INFO    -  Your documentation should shortly be available at: https://NFDI4Chem.github.io/VibrationSpectroscopyOntology/ \n</code></pre> 3. Just to double check, you can now navigate to your documentation pages (usually https://NFDI4Chem.github.io/VibrationSpectroscopyOntology/).     Just make sure you give GitHub 2-5 minutes to build the pages!</p>"},{"location":"odk-workflows/ReleaseWorkflow/","title":"The release workflow","text":"<p>The release workflow recommended by the ODK is based on GitHub releases and works as follows:</p> <ol> <li>Run a release with the ODK</li> <li>Review the release</li> <li>Merge to main branch</li> <li>Create a GitHub release</li> </ol> <p>These steps are outlined in detail in the following.</p>"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","title":"Run a release with the ODK","text":"<p>Preparation:</p> <ol> <li>Ensure that all your pull requests are merged into your main (master) branch</li> <li>Make sure that all changes to main are committed to GitHub (<code>git status</code> should say that there are no modified files)</li> <li>Locally make sure you have the latest changes from main (<code>git pull</code>)</li> <li>Checkout a new branch (e.g. <code>git checkout -b release-2021-01-01</code>)</li> <li>You may or may not want to refresh your imports as part of your release strategy (see here)</li> <li>Make sure you have the latest ODK installed by running <code>docker pull obolibrary/odkfull</code></li> </ol> <p>To actually run the release, you:</p> <ol> <li>Open a command line terminal window and navigate to the src/ontology directory (<code>cd vibso/src/ontology</code>)</li> <li>Run release pipeline:<code>sh run.sh make prepare_release -B</code>. Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI.</li> <li>If everything went well, you should see the following output on your machine: <code>Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab</code>.</li> </ol> <p>This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).</p>"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","title":"Review the release","text":"<ol> <li>(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (vibso.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: <ol> <li>Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly.</li> <li>Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. </li> </ol> </li> <li>Commit your changes to the branch and make a pull request</li> <li>In your GitHub pull request, review the following three files in detail (based on our experience):<ol> <li><code>vibso.obo</code> - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review!</li> <li><code>vibso-base.owl</code> - this reflects the asserted axioms in your ontology that you have actually edited.</li> <li>Ideally also take a look at <code>vibso-full.owl</code>, which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large.</li> </ol> </li> <li>Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!</li> </ol>"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","title":"Merge the main branch","text":"<p>Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).</p>"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","title":"Create a GitHub release","text":"<ol> <li>Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/NFDI4Chem/VibrationSpectroscopyOntology/releases). Then click \"Draft new release\"</li> <li>As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the <code>vibso.obo</code> file and check the <code>data-version:</code> property. The date needs to be prefixed with a <code>v</code>, so, for example <code>v2020-02-06</code>.</li> <li>You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions.</li> <li>Click \"Publish release\". Done.</li> </ol>"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","title":"Debugging typical ontology release problems","text":""},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","title":"Problems with memory","text":"<p>When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here.</p>"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","title":"Problems when using OBO format based tools","text":"<p>Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations (<code>def</code>, <code>comment</code>) which are illegal in OBO. Here is an example recipe of how to deal with such a problem:</p> <ol> <li>If you get a message like <code>make: *** [cl.Makefile:84: oort] Error 255</code> you might have a OORT error. </li> <li>To debug this, in your terminal enter <code>sh run.sh make IMP=false PAT=false oort -B</code> (assuming you are already in the ontology folder in your directory) </li> <li>This should show you where the error is in the log (eg multiple different definitions)  WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE</li> <li>Open <code>vibso-edit.owl</code> in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save.  *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in.</li> <li>Rerun <code>sh run.sh make IMP=false PAT=false oort -B</code> and if it all passes, commit your changes to a branch and make a pull request as usual.</li> </ol>"},{"location":"odk-workflows/RepoManagement/","title":"Managing your ODK repository","text":""},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","title":"Updating your ODK repository","text":"<p>Your ODK repositories configuration is managed in <code>src/ontology/vibso-odk.yaml</code>. The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository:</p> <pre><code>sh run.sh make update_repo\n</code></pre> <p>There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here.</p> <p>NOTE for Windows users:</p> <p>You may get a cryptic failure such as <code>Set Illegal Option -</code> if the update script located in <code>src/scripts/update_repo.sh</code>  was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can  click on Edit-&gt;EOL Conversion-&gt;Unix LF to change this.</p>"},{"location":"odk-workflows/RepoManagement/#managing-imports","title":"Managing imports","text":"<p>You can use the update repository workflow described on this page to perform the following operations to your imports:</p> <ol> <li>Add a new import</li> <li>Modify an existing import</li> <li>Remove an import you no longer want</li> <li>Customise an import</li> </ol> <p>We will discuss all these workflows in the following.</p>"},{"location":"odk-workflows/RepoManagement/#add-new-import","title":"Add new import","text":"<p>To add a new import, you first edit your odk config as described above, adding an <code>id</code> to the <code>product</code> list in the <code>import_group</code> section (for the sake of this example, we assume you already import RO, and your goal is to also import GO):</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n</code></pre> <p>Note: our ODK file should only have one <code>import_group</code> which can contain multiple imports (in the <code>products</code> section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here. To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps:</p> <ol> <li>Add an import statement to your <code>src/ontology/vibso-edit.owl</code> file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows:     <pre><code>...\nOntology(&lt;http://purl.obolibrary.org/obo/vibso.owl&gt;\nImport(&lt;http://purl.obolibrary.org/obo/vibso/imports/ro_import.owl&gt;)\nImport(&lt;http://purl.obolibrary.org/obo/vibso/imports/go_import.owl&gt;)\n...\n</code></pre></li> <li>Add your imports redirect to your catalog file <code>src/ontology/catalog-v001.xml</code>, for example:     <pre><code>&lt;uri name=\"http://purl.obolibrary.org/obo/vibso/imports/go_import.owl\" uri=\"imports/go_import.owl\"/&gt;\n</code></pre></li> <li>Test whether everything is in order:<ol> <li>Refresh your import</li> <li>Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported.</li> </ol> </li> </ol> <p>Note: The catalog file <code>src/ontology/catalog-v001.xml</code> has one purpose: redirecting  imports from URLs to local files. For example, if you have</p> <pre><code>Import(&lt;http://purl.obolibrary.org/obo/vibso/imports/go_import.owl&gt;)\n</code></pre> <p>in your editors file (the ontology) and</p> <pre><code>&lt;uri name=\"http://purl.obolibrary.org/obo/vibso/imports/go_import.owl\" uri=\"imports/go_import.owl\"/&gt;\n</code></pre> <p>in your catalog, tools like <code>robot</code> or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL <code>http://purl.obolibrary.org/obo/vibso/imports/go_import.owl</code> to the local file <code>imports/go_import.owl</code> (which is in your <code>src/ontology</code> directory).</p>"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","title":"Modify an existing import","text":"<p>If you simply wish to refresh your import in light of new terms, see here. If you wish to change the type of your module see section \"Customise an import\".</p>"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","title":"Remove an existing import","text":"<p>To remove an existing import, perform the following steps:</p> <ol> <li>remove the import declaration from your <code>src/ontology/vibso-edit.owl</code>.</li> <li>remove the id from your <code>src/ontology/vibso-odk.yaml</code>, eg. <code>- id: go</code> from the list of <code>products</code> in the <code>import_group</code>.</li> <li>run update repo workflow</li> <li>delete the associated files manually:<ul> <li><code>src/imports/go_import.owl</code></li> <li><code>src/imports/go_terms.txt</code></li> </ul> </li> <li>Remove the respective entry from the <code>src/ontology/catalog-v001.xml</code> file.</li> </ol>"},{"location":"odk-workflows/RepoManagement/#customise-an-import","title":"Customise an import","text":"<p>By default, an import module extracted from a source ontology will be a SLME module, see here. There are various options to change the default.</p> <p>The following change to your repo config (<code>src/ontology/vibso-odk.yaml</code>) will switch the go import from an SLME module to a simple ROBOT filter module:</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n      module_type: filter\n</code></pre> <p>A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the <code>filter</code> module does  not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This  behaviour can be changed by adding additional base IRIs as follows:</p> <pre><code>import_group:\n  products:\n    - id: go\n      module_type: filter\n      base_iris:\n        - http://purl.obolibrary.org/obo/GO_\n        - http://purl.obolibrary.org/obo/CL_\n        - http://purl.obolibrary.org/obo/BFO\n</code></pre> <p>If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config (<code>src/ontology/vibso-odk.yaml</code>):</p> <pre><code>import_group:\n  products:\n    - id: ro\n    - id: go\n      module_type: custom\n</code></pre> <p>Now add a new goal in your custom Makefile (<code>src/ontology/vibso.Makefile</code>, not <code>src/ontology/Makefile</code>).</p> <pre><code>imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt\n    if [ $(IMP) = true ]; then $(ROBOT) query  -i $&lt; --update ../sparql/preprocess-module.ru \\\n        extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\\n        query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\\n        annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl &amp;&amp; mv $@.tmp.owl $@; fi\n</code></pre> <p>Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only:</p> <pre><code>extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\\n</code></pre> <p>to another ROBOT pipeline.</p>"},{"location":"odk-workflows/RepoManagement/#add-a-component","title":"Add a component","text":"<p>A component is an import which belongs to your ontology, e.g. is managed by  you and your team. </p> <ol> <li>Open <code>src/ontology/vibso-odk.yaml</code></li> <li>If you dont have it yet, add a new top level section <code>components</code></li> <li>Under the <code>components</code> section, add a new section called <code>products</code>.  This is where all your components are specified</li> <li>Under the <code>products</code> section, add a new component, e.g. <code>- filename: mycomp.owl</code></li> </ol> <p>Example</p> <pre><code>components:\n  products:\n    - filename: mycomp.owl\n</code></pre> <p>When running <code>sh run.sh make update_repo</code>, a new file <code>src/ontology/components/mycomp.owl</code> will  be created which you can edit as you see fit. Typical ways to edit:</p> <ol> <li>Using a ROBOT template to generate the component (see below)</li> <li>Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor</li> <li>Providing a <code>components/mycomp.owl:</code> make target in <code>src/ontology/vibso.Makefile</code> and provide a custom command to generate the component<ul> <li><code>WARNING</code>: Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue).</li> </ul> </li> <li>Providing an additional attribute for the component in <code>src/ontology/vibso-odk.yaml</code>, <code>source</code>, to specify that this component should simply be downloaded from somewhere on the web.</li> </ol>"},{"location":"odk-workflows/RepoManagement/#adding-a-new-component-based-on-a-robot-template","title":"Adding a new component based on a ROBOT template","text":"<p>Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps:</p> <ol> <li>Open <code>src/ontology/vibso-odk.yaml</code>.</li> <li>Make sure that <code>use_templates: TRUE</code> is set in the global project options. You should also make sure that <code>use_context: TRUE</code> is set in case you are using prefixes in your templates that are not known to <code>robot</code>, such as <code>OMOP:</code>, <code>CPONT:</code> and more. All non-standard prefixes you are using should be added to <code>config/context.json</code>.</li> <li>Add another component to the <code>products</code> section.</li> <li>To activate this component to be template-driven, simply say: <code>use_template: TRUE</code>. This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. <code>run.bat make recreate-mycomp</code>).</li> <li>If you want to use more than one component, use the <code>templates</code> field to add as many template names as you wish. ODK will look for them in the <code>src/templates</code> directory.</li> <li>Advanced: If you want to provide additional processing options, you can use the <code>template_options</code> field. This should be a string with option from robot template. One typical example for additional options you may want to provide is <code>--add-prefixes config/context.json</code> to ensure the prefix map of your context is provided to <code>robot</code>, see above.</li> </ol> <p>Example:</p> <pre><code>components:\n  products:\n    - filename: mycomp.owl\n      use_template: TRUE\n      template_options: --add-prefixes config/context.json\n      templates:\n        - template1.tsv\n        - template2.tsv\n</code></pre> <p>Note: if your mirror is particularly large and complex, read this ODK recommendation.</p>"},{"location":"odk-workflows/RepositoryFileStructure/","title":"Repository structure","text":"<p>The main kinds of files in the repository:</p> <ol> <li>Release files</li> <li>Imports</li> <li>Components</li> </ol>"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","title":"Release files","text":"<p>Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here.</p>"},{"location":"odk-workflows/RepositoryFileStructure/#imports","title":"Imports","text":"<p>Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain.</p> <p>These are the current imports in VIBSO</p> Import URL Type bfo http://purl.obolibrary.org/obo/bfo.owl mirror ro http://purl.obolibrary.org/obo/ro.owl custom omo http://purl.obolibrary.org/obo/omo.owl mirror iao http://purl.obolibrary.org/obo/iao.owl None chmo http://purl.obolibrary.org/obo/chmo.owl custom pato http://purl.obolibrary.org/obo/pato.owl None obi http://purl.obolibrary.org/obo/obi.owl custom stato http://purl.obolibrary.org/obo/stato.owl None uo http://purl.obolibrary.org/obo/uo.owl custom chebi http://purl.obolibrary.org/obo/chebi.owl None foodon http://purl.obolibrary.org/obo/foodon.owl None"},{"location":"odk-workflows/RepositoryFileStructure/#components","title":"Components","text":"<p>Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases:</p> <ol> <li>There is an automated process that generates and re-generates a part of the ontology</li> <li>A part of the ontology is managed in ROBOT templates</li> <li>The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component.</li> </ol> <p>These are the components in VIBSO</p> Filename URL vibso_classes.owl None vibso_object_properties.owl None vibso_examples.owl None"},{"location":"odk-workflows/SettingUpDockerForODK/","title":"Setting up your Docker environment for ODK use","text":"<p>One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception,  but more often than not it will appear as something like an <code>Error 137</code>. There are two places you need to consider to set your memory:</p> <ol> <li>Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding  <code>robot_java_args: '-Xmx8G'</code> to your src/ontology/vibso-odk.yaml file, see for example here.</li> <li>Set your docker memory. By default, it should be about 10-20% more than your <code>robot_java_args</code> variable. You can manage your memory settings by right-clicking on the docker whale in your system bar--&gt;Preferences--&gt;Resources--&gt;Advanced, see picture below.</li> </ol> <p></p>"},{"location":"odk-workflows/UpdateImports/","title":"Update Imports Workflow","text":"<p>This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here.</p>"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","title":"Importing a new term","text":"<p>Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\".</p> <p>Importing a new term is split into two sub-phases:</p> <ol> <li>Declaring the terms to be imported</li> <li>Refreshing imports dynamically</li> </ol>"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","title":"Declaring terms to be imported","text":"<p>There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be):</p> <ol> <li>Prot\u00e9g\u00e9-based declaration</li> <li>Using term files</li> <li>Using the custom import template</li> </ol>"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","title":"Prot\u00e9g\u00e9-based declaration","text":"<p>This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container.  This approach also applies to ontologies that use base module import approach.</p> <ol> <li>Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+).</li> <li>Select 'owl:Thing'</li> <li>Add a new class as usual.</li> <li>Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906.</li> <li>Click 'OK'</li> </ol> <p></p> <p>Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology.</p>"},{"location":"odk-workflows/UpdateImports/#using-term-files","title":"Using term files","text":"<p>Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in <code>src/ontology/go_import.owl</code>, you will also have an associated term file <code>src/ontology/go_terms.txt</code>. You can add terms in there simply as a list:</p> <pre><code>GO:0008150\nGO:0008151\n</code></pre> <p>Now you can run the refresh imports workflow) and the two terms will be imported.</p>"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","title":"Using the custom import template","text":"<p>This workflow is appropriate if:</p> <ol> <li>You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above).</li> <li>You wish to augment your imported ontologies with additional information. This requires a cautionary discussion.</li> </ol> <p>To enable this workflow, you add the following to your ODK config file (<code>src/ontology/vibso-odk.yaml</code>), and update the repository:</p> <pre><code>use_custom_import_module: TRUE\n</code></pre> <p>Now you can manage your imported terms directly in the custom external terms template, which is located at <code>src/templates/external_import.owl</code>. Note that this file is a ROBOT template, and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully.</p> <p>The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import <code>APOLLO_SV:00000480</code>, and you wish to import <code>APOLLO_SV:00000532</code>, you simply add a row like this:</p> <pre><code>ID  Entity Type\nID  TYPE\nAPOLLO_SV:00000480  owl:Class\nAPOLLO_SV:00000532  owl:Class\n</code></pre> <p>When the imports are refreshed see imports refresh workflow, the term(s) will simply be imported from the configured ontologies.</p> <p>Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the <code>ID</code> and <code>ENTITY</code> columns and (b) ensure that the ROBOT template is valid otherwise, see here.</p> <p>WARNING. Note that doing this is a widespread antipattern (see related issue). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file.  </p>"},{"location":"odk-workflows/UpdateImports/#refresh-imports","title":"Refresh imports","text":"<p>If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example):</p> <p>First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).  <pre><code>cd src/ontology\n</code></pre></p> <p>Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed.</p> <pre><code>sh run.sh make PAT=false imports/go_import.owl -B\n</code></pre> <p>Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above:</p> <pre><code>sh run.sh make refresh-go\n</code></pre> <p>Note that in case you changed the defaults, you need to add <code>IMP=true</code> and/or <code>MIR=true</code> to the command below:</p> <pre><code>sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B\n</code></pre> <p>If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. <code>go.owl</code> for your go import) you can set <code>MIR=false</code> instead, which will do the exact same thing as the above, but is easier to remember:</p> <pre><code>sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B\n</code></pre>"},{"location":"odk-workflows/UpdateImports/#using-the-base-module-approach","title":"Using the Base Module approach","text":"<p>Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it.  A base file is a subset of the ontology that only contains those axioms that nominally  belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this:</p> <p>Imagine this being the full Uberon ontology:</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\nAxiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>The base file is the set of all axioms that are about UBERON terms:</p> <pre><code>Axiom 1: UBERON:123 SubClassOf BFO:123\nAxiom 1: UBERON:124 SubClassOf UBERON 123\n</code></pre> <p>I.e.</p> <pre><code>Axiom 1: BFO:123 SubClassOf BFO:124\n</code></pre> <p>Gets removed.</p> <p>The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first  merging all mirrors into one huge file and then extracting one mega module from it.</p> <p>Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we</p> <p>1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module <code>imports/merged_import.owl</code></p> <p>The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml.</p> <p>To check if your ontology uses this method, check src/ontology/vibso-odk.yaml to see if <code>use_base_merging: TRUE</code> is declared under <code>import_group</code></p> <p>If your ontology uses Base Module approach, please use the following steps: </p> <p>First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you)</p> <p>Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory).  <pre><code>cd src/ontology\n</code></pre></p> <p>Then refresh imports by running</p> <p><pre><code>sh run.sh make imports/merged_import.owl\n</code></pre> Note: if your mirrors are updated, you can run <code>sh run.sh make no-mirror-refresh-merged</code></p> <p>This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you.</p> <p>Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.</p>"},{"location":"odk-workflows/components/","title":"Adding components to an ODK repo","text":"<p>For details on what components are, please see component section of repository file structure document.</p> <p>To add custom components to an ODK repo, please follow the following steps:</p> <p>1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/vibso-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component:</p> <pre><code>components:\n  products:\n    - filename: your-component-name.owl\n</code></pre> <p>3) Add the component to your catalog file (src/ontology/catalog-v001.xml)</p> <pre><code>  &lt;uri name=\"http://purl.obolibrary.org/obo/vibso/components/your-component-name.owl\" uri=\"components/your-component-name.owl\"/&gt;\n</code></pre> <p>4) Add the component to the edit file (src/ontology/vibso-edit.obo) for .obo formats: </p> <pre><code>import: http://purl.obolibrary.org/obo/vibso/components/your-component-name.owl\n</code></pre> <p>for .owl formats: </p> <pre><code>Import(&lt;http://purl.obolibrary.org/obo/vibso/components/your-component-name.owl&gt;)\n</code></pre> <p>5) Refresh your repo by running <code>sh run.sh make update_repo</code> - this should create a new file in src/ontology/components. 6) In your custom makefile (src/ontology/vibso.Makefile) add a goal for your custom make file. In this example, the goal is a ROBOT template.</p> <pre><code>$(COMPONENTSDIR)/your-component-name.owl: $(SRC) ../templates/your-component-template.tsv \n    $(ROBOT) template --template ../templates/your-component-template.tsv \\\n  annotate --ontology-iri $(ONTBASE)/$@ --output $(COMPONENTSDIR)/your-component-name.owl\n</code></pre> <p>(If using a ROBOT template, do not forget to add your template tsv in src/templates/)</p> <p>7) Make the file by running <code>sh run.sh make components/your-component-name.owl</code></p>"}]}